{#-
 # Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 # Copyright (C) 2021  UAVCAN Development Team  <uavcan.org>
 # This software is distributed under the terms of the MIT License.
 # Authors: David Lenfesty, Scott Dixon <dixonsco@amazon.com>, Pavel Kirienko <pavel@uavcan.org>,
 #          Peter van der Perk <peter.vanderperk@nxp.com>, Pavel Pletenev <cpp.create@gmail.com>
-#}

{%- macro assert(expression) -%}
    {%- if options.enable_serialization_asserts -%}
    NUNAVUT_ASSERT({{ expression }});
    {%- endif -%}
{%- endmacro -%}

// UAVCAN common serialization support routines.                                                             +-+ +-+
// This file is based on canard_dsdl.h, which is part of Libcanard.                                          | | | |
//                                                                                                           \  -  /
// AUTOGENERATED, DO NOT EDIT.                                                                                 ---
//                                                                                                              o
//---------------------------------------------------------------------------------------------------------------------
// Language Options
{% for key, value in options.items() -%}
//     {{ key }}:  {{ value }}
{% endfor %}

#ifndef NUNAVUT_SUPPORT_SERIALIZATION_HPP_INCLUDED
#define NUNAVUT_SUPPORT_SERIALIZATION_HPP_INCLUDED

static_assert(__cplusplus >= 201100L,
              "Unsupported language: ISO C11, C++11, or a newer version of either is required.");

#include <cstring> // for std::size_t
{% if not options.omit_float_serialization_support %}
#include <cmath>  // For isfinite().
{% endif -%}
#include <cstdint>
#include "span_lite.hpp"
#include "tl_expected.hpp"

static_assert(sizeof({{ typename_unsigned_bit_length }}) >= sizeof({{ typename_unsigned_length }}),
    "The bit-length type used by Nunavut, {{ typename_unsigned_bit_length }}, "
    "is smaller than this platform's {{ typename_unsigned_length }} type. "
    "Nunavut serialization relies on {{ typename_unsigned_length }} to {{ typename_unsigned_bit_length }} conversions "
    "that do not lose data. You will need to regenerate Nunavut serialization support with a larger "
    "unsigned_bit_length type specified.");

namespace nunavut
{
namespace support
{

using nonstd::span;
using bytespan = span<{{ typename_byte }}> ;
using const_bytespan = span<const {{ typename_byte }}> ;
#if span_FEATURE( MAKE_SPAN )
using nonstd::make_span;
#endif  // span_FEATURE( MAKE_SPAN )


/// Nunavut serialization will never define more than 127 errors and the reserved error numbers are [1,127]
/// (128 is not used). Error code 1 is currently also not used to avoid conflicts with 3rd-party software.
enum class Error{
    // API usage errors:
    SERIALIZATION_INVALID_ARGUMENT = 2,
    SERIALIZATION_BUFFER_TOO_SMALL = 3,
    // Invalid representation (caused by bad input data, not API misuse):
    REPRESENTATION_BAD_ARRAY_LENGTH=10,
    REPRESENTATION_BAD_UNION_TAG=11,
    REPRESENTATION_BAD_DELIMITER_HEADER=12
};

inline tl::unexpected<Error> operator-(const Error& e){
    return tl::unexpected<Error>{e};
}

template<typename Ret>
using Result = tl::expected<Ret, Error>;
using VoidResult = Result<void>;
using SerializeResult = Result<{{ typename_unsigned_length }}>;

namespace options
{
{% for key, value in options.items() -%}
constexpr std::uint32_t {{ key | id }} = {{ value | ln.c.to_static_assertion_value }};
{% endfor %}
} // end namespace options



// ---------------------------------------------------- BIT ARRAY ----------------------------------------------------
namespace detail{
template<typename derived_bitspan>
struct any_bitspan{
protected:
    const uint8_t* unchecked_aligned_ptr(std::size_t plus_offset_bits=0U) const noexcept {
        auto& self  = *static_cast<const derived_bitspan*>(this);
        const std::size_t offset_bytes = ((self.offset_bits_ + plus_offset_bits) / 8U);
        return self.data_.data() + offset_bytes;
    }
public:
    derived_bitspan at_offset({{ typename_unsigned_bit_length }} bits) const noexcept {
        auto& self  = *static_cast<const derived_bitspan*>(this);
        return derived_bitspan(self.data_, self.offset_bits_ + bits);
    }

    void add_offset({{ typename_unsigned_bit_length }} bits) noexcept{
        auto& self  = *static_cast<derived_bitspan*>(this);
        self.offset_bits_ += bits;
    }

    void set_offset({{ typename_unsigned_bit_length }} bits) noexcept{
        auto& self  = *static_cast<derived_bitspan*>(this);
        self.offset_bits_ = bits;
    }

    {{ typename_unsigned_bit_length }} size() const noexcept{
        auto& self  = *static_cast<const derived_bitspan*>(this);
        {{ typename_unsigned_bit_length }} bit_size = {# -#}
            static_cast<{{ typename_unsigned_bit_length }}>(self.data_.size()) * 8U;
        if(bit_size < self.offset_bits_){
            return 0U;
        }
        return bit_size - self.offset_bits_;
    }

    {{ typename_byte }}& aligned_ref({{ typename_unsigned_length }} plus_offset_bits=0U) noexcept {
        auto& self  = *static_cast<derived_bitspan*>(this);
        const {{ typename_unsigned_length }} offset_bytes = ((self.offset_bits_ + plus_offset_bits) / 8U);
        {{ assert('offset_bytes <= self.data_.size()') }}
        return self.data_[offset_bytes];
    }

    const {{ typename_byte }}& aligned_ref({{ typename_unsigned_length }} plus_offset_bits=0U) const noexcept {
        auto& self  = *static_cast<const derived_bitspan*>(this);
        const {{ typename_unsigned_length }} offset_bytes = ((self.offset_bits_ + plus_offset_bits) / 8U);
        {{ assert('offset_bytes <= self.data_.size()') }}
        return self.data_[offset_bytes];
    }

    {{ typename_byte }}* aligned_ptr({{ typename_unsigned_length }} plus_offset_bits=0U) noexcept {
        return &aligned_ref(plus_offset_bits);
    }

    const {{ typename_byte }}* aligned_ptr({{ typename_unsigned_length }} plus_offset_bits=0U) const noexcept {
        return &aligned_ref(plus_offset_bits);
    }
};

} // namespace detail

struct bitspan: public detail::any_bitspan<bitspan>{
    friend class detail::any_bitspan<bitspan>;
    friend class const_bitspan;
private:
    bytespan data_;
    {{ typename_unsigned_bit_length }} offset_bits_;
public:
    bitspan(bytespan data, {{ typename_unsigned_bit_length }} offset_bits=0)
        :data_(std::move(data)), offset_bits_(offset_bits){
    }
    // ---------------------------------------------------- INTEGER ----------------------------------------------------
    /// Serialize a DSDL field value at the specified bit offset from the beginning of the destination buffer.
    /// The behavior is undefined if the input pointer is nullprt. The time complexity is linear of the bit length.
    /// One-bit-wide signed integers are processed without raising an error but the result is unspecified.
    ///
    /// Arguments:
    ///     value           The value itself (in case of integers it is promoted to 64-bit for unification).
    ///     len_bits        Length of the serialized representation, in bits. Zero has no effect. Values >64 bit saturated.
    VoidResult setBit(const bool value);

    VoidResult setUxx(const uint64_t value, const uint8_t len_bits);

    VoidResult setIxx(const int64_t value, const uint8_t len_bits);
};

struct const_bitspan: public detail::any_bitspan<const_bitspan>{
    friend class detail::any_bitspan<const_bitspan>;
private:
    const_bytespan data_;
    {{ typename_unsigned_bit_length }} offset_bits_;
public:
    const_bitspan(const_bytespan data, {{ typename_unsigned_bit_length }} offset_bits=0)
        :data_(std::move(data)), offset_bits_(offset_bits){
    }
    /// Copy the specified number of bits from the source buffer into the destination buffer in accordance with the
    /// DSDL bit-level serialization specification. The offsets may be arbitrary (may exceed 8 bits).
    /// If both offsets are byte-aligned, the function invokes memmove() and possibly adjusts the last byte separately.
    /// If the source and the destination overlap AND the offsets are not byte-aligned, the behavior is undefined.
    /// If either source or destination pointers are NULL, the behavior is undefined.
    /// Arguments:
    ///     dst         Destination buffer. Shall be at least ceil(length_bits/8) bytes large.
    ///     length_bits The number of bits to copy. Both source and destination shall be large enough.
    void copyTo(bitspan dst){copyTo(dst, size());}
    void copyTo({# -#}
            bitspan dst,{# -#}
            {{ typename_unsigned_bit_length }} length_bits) const noexcept {
        {{ assert('data_.data() != nullptr') }}
        {{ assert('dst.data_.data() != nullptr') }}
        {{ assert('data_.data() != dst.data_.data()') }}
        if(length_bits > size()){
            length_bits = size();
        }
        if(length_bits == 0){
            return;
        }
        {{ assert('length_bits <= dst.size()') }}
        if ((0U == (offset_bits_ % 8U)) && (0U == (dst.offset_bits_ % 8U)))  // Aligned copy, optimized, most common case.
        {
            const {{ typename_unsigned_length }} length_bytes = static_cast<{{ typename_unsigned_length }}>(length_bits / 8U);

            (void) memmove(dst.aligned_ptr(), aligned_ptr(), length_bytes);
            const uint8_t length_mod = static_cast<uint8_t>(length_bits % 8U);
            if (0U != length_mod)  // If the length is unaligned, the last byte requires special treatment.
            {
                {{ assert('length_mod < 8U') }}
                const uint8_t mask = static_cast<uint8_t>((1U << length_mod) - 1U);
                dst.data_[length_bytes] = (dst.data_[length_bytes] & static_cast<{{ typename_byte }}>(~mask)) | (data_[length_bytes] & mask);
            }
        }
        else
        {
            // The algorithm was originally designed by Ben Dyer for Libuavcan v0:
            // https://github.com/UAVCAN/libuavcan/blob/legacy-v0/libuavcan/src/marshal/uc_bit_array_copy.cpp
            // This version is modified for v1 where the bit order is the opposite.
            {{ typename_unsigned_bit_length }}       src_off  = offset_bits_;
            {{ typename_unsigned_bit_length }}       dst_off  = dst.offset_bits_;
            const {{ typename_unsigned_bit_length }} last_bit = src_off + length_bits;
            {{ assert(
                '((aligned_ptr() < dst.aligned_ptr()) ? (unchecked_aligned_ptr(length_bits) <= dst.aligned_ptr()) : true)'
            ) }}
            {{ assert(
                '((aligned_ptr() > dst.aligned_ptr()) ? (dst.unchecked_aligned_ptr(length_bits) <= aligned_ptr()) : true)'
            ) }}
            while (last_bit > src_off)
            {
                const uint8_t src_mod = (src_off % 8U);
                const uint8_t dst_mod = (dst_off % 8U);
                const uint8_t max_mod = (src_mod > dst_mod) ? src_mod : dst_mod;
                const {{ typename_unsigned_bit_length }} max_mod_inv = 8U - max_mod;
                const {{ typename_unsigned_bit_length }} last_off = last_bit - src_off;
                const uint8_t size = static_cast<uint8_t>(std::min(max_mod_inv, last_off));
                {{ assert('size > 0U') }}
                {{ assert('size <= 8U') }}
                // Suppress a false warning from Clang-Tidy & Sonar that size is being over-shifted. It's not.
                const uint8_t mask = ((((1U << size) - 1U) << dst_mod) & 0xFFU);  // NOLINT NOSONAR
                {{ assert('mask > 0U') }}
                // Intentional violation of MISRA: indexing on a pointer.
                // This simplifies the implementation greatly and avoids pointer arithmetics.
                const uint8_t in = static_cast<uint8_t>(static_cast<uint8_t>(data_[src_off / 8U] >> src_mod) << dst_mod) & 0xFFU;  // NOSONAR
                // Intentional violation of MISRA: indexing on a pointer.
                // This simplifies the implementation greatly and avoids pointer arithmetics.
                const uint8_t a = dst.data_[dst_off / 8U] & (static_cast<uint8_t>(~mask));  // NOSONAR
                const uint8_t b = in & mask;
                // Intentional violation of MISRA: indexing on a pointer.
                // This simplifies the implementation greatly and avoids pointer arithmetics.
                dst.data_[dst_off / 8U] = a | b;  // NOSONAR
                src_off += size;
                dst_off += size;
            }
            {{ assert('last_bit == src_off') }}
        }
    }

    /// Calculate the number of bits to safely copy from/to a serialized buffer.
    /// Mind the units! By convention, buffer size is specified in bytes, but fragment length and offset are in bits.
    ///
    ///      buffer                                                                buffer
    ///      origin                                                                 end
    ///         [------------------------------ data_.size() ------------------------]
    ///         [-------------------- offset_bits_ ------------------][--- fragment_length_bits ---]
    ///                                                               [-- out bits --]
    ///                                                                              ^
    ///                                       this position is returned -------------/
    ///
    {{ typename_unsigned_bit_length }} saturateBufferFragmentBitLength({# -#}
        const {{ typename_unsigned_bit_length }} fragment_length_bits) const noexcept
    {
        const {{ typename_unsigned_bit_length }} size_bits = static_cast<{{ typename_unsigned_bit_length }}>(data_.size()) * 8U;
        const {{ typename_unsigned_bit_length }} tail_bits = size_bits - std::min(size_bits, offset_bits_);
        return std::min(fragment_length_bits, tail_bits);
    }

    /// This function is intended for deserialization of contiguous sequences of zero-cost primitives.
    /// It extracts (len_bits) bits that are offset by (offset_bits_) from the origin of (data_) whose size is (data_.size()).
    /// If the requested (len_bits+offset_bits_) overruns the buffer, the missing bits are implicitly zero-extended.
    /// If (len_bits % 8 != 0), the output buffer is right-zero-padded up to the next byte boundary.
    /// If (off_bits % 8 == 0), the operation is delegated to memmove(); otherwise, a much slower unaligned bit copy
    /// algorithm is employed. See @ref nunavutCopyBits() for further details.
    void getBits(bytespan output, const {{ typename_unsigned_bit_length }} len_bits)
    {
        {{ assert('output.data() != nullptr') }}
        {{ assert('data_.data() != nullptr') }}
        const {{ typename_unsigned_length }} len_bytes = (len_bits + 7U) / 8U;
        {{ assert('output.size() >= len_bytes') }}
        const {{ typename_unsigned_bit_length }} sat_bits = saturateBufferFragmentBitLength(len_bits);
        // Apply implicit zero extension. Normally, this is a no-op unless (len_bits > sat_bits) or (len_bits % 8 != 0).
        // The former case ensures that if we're copying <8 bits, the MSB in the destination will be zeroed out.
        std::memset(output.data() + (sat_bits / 8U), 0, len_bytes - (sat_bits / 8U));
        copyTo(bitspan{output, 0U}, sat_bits);
    }



    /// Deserialize a DSDL field value located at the specified bit offset from the beginning of the source buffer.
    /// If the deserialized value extends beyond the end of the buffer, the missing bits are taken as zero, as required
    /// by the DSDL specification (see Implicit Zero Extension Rule, IZER).
    ///
    /// If len_bits is greater than the return type, extra bits will be truncated per standard narrowing conversion rules.
    /// If len_bits is shorter than the return type, missing bits will be zero per standard integer promotion rules.
    /// Essentially, for integers, it would be enough to have 64-bit versions only; narrower variants exist only to avoid
    /// narrowing type conversions of the result and for some performance gains.
    ///
    /// The behavior is undefined if the input pointer is NULL. The time complexity is linear of the bit length.
    /// One-bit-wide signed integers are processed without raising an error but the result is unspecified.
    ///
    /// Arguments:
    ///     len_bits        Length of the serialized representation, in bits. Zero returns 0. Out-of-range values saturated.

    bool getBit() const noexcept
    {
        return 1U == getU8(1U);
    }

    uint8_t getU8(const uint8_t len_bits) const noexcept
    {
        {{ assert('data_.data() != nullptr') }}
        const {{ typename_unsigned_bit_length }} bits = saturateBufferFragmentBitLength(std::min<uint8_t>(len_bits, 8U));
        {{ assert('bits <= (sizeof(uint8_t) * 8U)') }}
        uint8_t val = 0;
        copyTo(bitspan{ { &val, 1U } }, bits);
        return val;
    }

    uint16_t getU16(const uint8_t len_bits) const noexcept
    {
        {{ assert('data_.data() != nullptr') }}
        const {{ typename_unsigned_bit_length }} bits = saturateBufferFragmentBitLength(std::min<uint8_t>(len_bits, 16U));
        {{ assert('bits <= (sizeof(uint16_t) * 8U)') }}
    {%- if options.target_endianness == 'little' %}
        uint16_t val = 0U;
        copyTo(bitspan{ { reinterpret_cast<uint8_t*>(&val), sizeof(val) } }, bits);
        return val;
    {%- elif options.target_endianness in ('any', 'big') %}
        uint8_t tmp[sizeof(uint16_t)] = {0};
        copyTo(bitspan{ { &tmp[0], sizeof(tmp) } }, bits);
        return static_cast<uint16_t>(static_cast<uint16_t>(tmp[0]) | ((static_cast<uint16_t>(tmp[1])) << 8U));
    {%- else %}{%- assert False %}
    {%- endif %}
    }

    uint32_t getU32(const uint8_t len_bits) const noexcept
    {
        {{ assert('data_.data() != nullptr') }}
        const {{ typename_unsigned_bit_length }} bits = saturateBufferFragmentBitLength(std::min<uint8_t>(len_bits, 32U));
        {{ assert('bits <= (sizeof(uint32_t) * 8U)') }}
    {%- if options.target_endianness == 'little' %}
        uint32_t val = 0U;
        copyTo(bitspan{ { reinterpret_cast<uint8_t*>(&val), sizeof(val) } }, bits);
        return val;
    {%- elif options.target_endianness in ('any', 'big') %}
        uint8_t tmp[sizeof(uint32_t)] = {0};
        copyTo(bitspan{ { &tmp[0], sizeof(tmp) } }, bits);
        return static_cast<uint32_t>(static_cast<uint32_t>(tmp[0]) |
                        (static_cast<uint32_t>(tmp[1]) << 8U) |
                        (static_cast<uint32_t>(tmp[2]) << 16U) |
                        (static_cast<uint32_t>(tmp[3]) << 24U));
    {%- else %}{%- assert False %}
    {%- endif %}
    }

    uint64_t getU64(const uint8_t len_bits) const noexcept
    {
        {{ assert('data_.data() != nullptr') }}
        const {{ typename_unsigned_bit_length }} bits = saturateBufferFragmentBitLength(std::min<uint8_t>(len_bits, 64U));
        {{ assert('bits <= (sizeof(uint64_t) * 8U)') }}
    {%- if options.target_endianness == 'little' %}
        uint64_t val = 0U;
        copyTo(bitspan{ { reinterpret_cast<uint8_t*>(&val), sizeof(val) } }, bits);
        return val;
    {%- elif options.target_endianness in ('any', 'big') %}
        uint8_t tmp[sizeof(uint64_t)] = {0};
        copyTo(bitspan{ { &tmp[0], sizeof(tmp) } }, bits);
        return static_cast<uint64_t>(static_cast<uint64_t>(tmp[0]) |
                        (static_cast<uint64_t>(tmp[1]) << 8U) |
                        (static_cast<uint64_t>(tmp[2]) << 16U) |
                        (static_cast<uint64_t>(tmp[3]) << 24U) |
                        (static_cast<uint64_t>(tmp[4]) << 32U) |
                        (static_cast<uint64_t>(tmp[5]) << 40U) |
                        (static_cast<uint64_t>(tmp[6]) << 48U) |
                        (static_cast<uint64_t>(tmp[7]) << 56U));
    {%- else %}{%- assert False %}
    {%- endif %}
    }

    int8_t getI8(const uint8_t len_bits) const noexcept
    {
        const uint8_t sat = std::min<uint8_t>(len_bits, 8U);
        uint8_t       val = getU8(sat);
        const bool    neg = (sat > 0U) && ((val & (1ULL << (sat - 1U))) != 0U);
        val = ((sat < 8U) && neg) ? static_cast<uint8_t>(val | ~((1U << sat) - 1U)) : val;  // Sign extension
        return neg ? static_cast<int8_t>(-static_cast<int8_t>(static_cast<uint8_t>(~val)) - 1) : static_cast<int8_t>(val);
    }

    int16_t getI16(const uint8_t len_bits) const noexcept
    {
        const uint8_t sat = std::min<uint8_t>(len_bits, 16U);
        uint16_t      val = getU16(sat);
        const bool    neg = (sat > 0U) && ((val & (1ULL << (sat - 1U))) != 0U);
        val = ((sat < 16U) && neg) ? static_cast<uint16_t>(val | ~((1U << sat) - 1U)) : val;  // Sign extension
        return neg ? static_cast<int16_t>(-static_cast<int16_t>(static_cast<uint16_t>(~val)) - 1) : static_cast<int16_t>(val);
    }

    int32_t getI32(const uint8_t len_bits) const noexcept
    {
        const uint8_t sat = std::min<uint8_t>(len_bits, 32U);
        uint32_t      val = getU32(sat);
        const bool    neg = (sat > 0U) && ((val & (1ULL << (sat - 1U))) != 0U);
        val = ((sat < 32U) && neg) ? static_cast<uint32_t>(val | ~((1UL << sat) - 1U)) : val;  // Sign extension
        return neg ? static_cast<int32_t>((-static_cast<int32_t>(~val)) - 1) : static_cast<int32_t>(val);
    }

    int64_t getI64(const uint8_t len_bits) const noexcept
    {
        const uint8_t sat = std::min<uint8_t>(len_bits, 64U);
        uint64_t      val = getU64(sat);
        const bool    neg = (sat > 0U) && ((val & (1ULL << (sat - 1U))) != 0U);
        val = ((sat < 64U) && neg) ? static_cast<uint64_t>(val | ~((1ULL << sat) - 1U)) : val;  // Sign extension
        return neg ? static_cast<int64_t>((-static_cast<int64_t>(~val)) - 1) : static_cast<int64_t>(val);
    }
};

VoidResult bitspan::setBit(const bool value)
{
    if ((data_.size() * 8U) <= offset_bits_)
    {
        return -Error::SERIALIZATION_BUFFER_TOO_SMALL;
    }
    const uint8_t val = value ? 1U : 0U;
    const_bitspan{ { &val, 1U } }.copyTo(*this, 1U);
    return {};
}

VoidResult bitspan::setUxx(const uint64_t value, const uint8_t len_bits)
{
    static_assert(64U == (sizeof(uint64_t) * 8U), "Unexpected size of uint64_t");
    if ((data_.size() * 8) < (offset_bits_ + len_bits))
    {
        return -Error::SERIALIZATION_BUFFER_TOO_SMALL;
    }
    const {{ typename_unsigned_bit_length }} saturated_len_bits = std::min<{{ typename_unsigned_bit_length }}>({# -#}
        len_bits, 64U);
{%- if options.target_endianness == 'little' %}
        bitspan{ { reinterpret_cast<const uint8_t*>(&value), sizeof(uint64_t) } }.copyTo(*this, saturated_len_bits);
{%- elif options.target_endianness in ('any', 'big') %}
    std::array<uint8_t, 8> tmp{
        static_cast<uint8_t>((value >> 0U) & 0xFFU),
        static_cast<uint8_t>((value >> 8U) & 0xFFU),
        static_cast<uint8_t>((value >> 16U) & 0xFFU),
        static_cast<uint8_t>((value >> 24U) & 0xFFU),
        static_cast<uint8_t>((value >> 32U) & 0xFFU),
        static_cast<uint8_t>((value >> 40U) & 0xFFU),
        static_cast<uint8_t>((value >> 48U) & 0xFFU),
        static_cast<uint8_t>((value >> 56U) & 0xFFU),
    };
    const_bitspan{ { &tmp[0], sizeof(uint64_t) } }.copyTo(*this, saturated_len_bits);
{%- else %}{%- assert False %}
{%- endif %}
    return {};
}

VoidResult bitspan::setIxx(const int64_t value, const uint8_t len_bits)
{
    // The naive sign conversion is safe and portable according to the C++ Standard 4.7/2 :
    // If the destination type is unsigned, the resulting value is the least unsigned integer
    // congruent to the source integer (modulo 2^n where n is the number of bits used to represent
    // the unsigned type). [Note: In a two's complement representation, this conversion is conceptual and there
    // is no change in the bit pattern (if there is no truncation). ]
    return setUxx(static_cast<uint64_t>(value), len_bits);
}


} // end namespace support
} // end namespace nunavut

#endif // NUNAVUT_SUPPORT_SERIALIZATION_HPP_INCLUDED
